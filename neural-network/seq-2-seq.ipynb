{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import torch\n",
    "import re\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.__version__, keras.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(paths):\n",
    "    total_words = 0\n",
    "\n",
    "    for path in paths:\n",
    "        book = open(path, 'rb').read().decode(encoding='utf-8').lower()\n",
    "        words = book.split()\n",
    "        print(f'{path} - Words: {len(words)}')\n",
    "        total_words += len(words)\n",
    "\n",
    "    print(f'Total Words: {total_words}')\n",
    "    \n",
    "    # Extrae palabras y signos de puntuación de un texto.\n",
    "    words = re.findall(r'\\b\\w+\\b|[\\.,;!?()\"\\']', book)\n",
    "\n",
    "    # Genera pares de secuencias de palabras y sus subsecuencias para entrenamiento.\n",
    "    maxlen = 15\n",
    "    text_pairs = []\n",
    "    for i in range(0, len(words), maxlen):\n",
    "        inp = words[i:i + maxlen]\n",
    "        out = words[i + maxlen :i + maxlen * 2]\n",
    "        text_pairs.append((' '.join(inp), ' '.join(out)))\n",
    "        for j in range(maxlen - 1):\n",
    "            text_pairs.append((' '.join(inp[j + 1:]), ' '.join(out)))\n",
    "\n",
    "    # Verificando cómo quedó text_pairs\n",
    "    for i in range(5):\n",
    "        print(text_pairs[i])\n",
    "    \n",
    "    return text_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "text_pairs = process_text(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# Crea un tokenizador de spaCy en inglés.\n",
    "eng_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se construye un vocabulario a partir de un conjunto de texto, contando las frecuencias de las palabras y filtrando aquellas que no alcanzan un umbral mínimo\n",
    "\n",
    "def build_vocab(text, tokenizers, min_freq=5):\n",
    "    eng_tokenizer = tokenizers\n",
    "    eng_counter = Counter()\n",
    "    for eng_string_prev_, eng_string_post_ in text:\n",
    "        eng_counter.update(eng_tokenizer(eng_string_prev_))\n",
    "        eng_counter.update(eng_tokenizer(eng_string_post_))\n",
    "    eng_vocab = Vocab(eng_counter, min_freq=min_freq, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "    return eng_vocab, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab, _ = build_vocab(text_pairs, eng_tokenizer, min_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eng_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir pares de texto en secuencias tensoriales de índices de vocabulario\n",
    "\n",
    "def data_process(text,maxlen=15):\n",
    "    data = []\n",
    "    for eng_prev, eng_post in text:\n",
    "        eng_prev_tensor_ = torch.tensor([eng_vocab[token] for token in eng_tokenizer(eng_prev)],\n",
    "                                dtype=torch.long)\n",
    "        eng_post_tensor_ = torch.tensor([eng_vocab[token] for token in eng_tokenizer(eng_post)],\n",
    "                                dtype=torch.long)\n",
    "\n",
    "        if eng_prev_tensor_.shape[0] < maxlen + 1:\n",
    "            data.append((eng_prev_tensor_, eng_post_tensor_))\n",
    "        else:\n",
    "            print(0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(text_pairs, maxlen=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "PAD_IDX = eng_vocab['<pad>']\n",
    "BOS_IDX = eng_vocab['<bos>']\n",
    "EOS_IDX = eng_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizar un lote de datos para el entrenamiento, agrega tokens de inicio y fin a las salidas, iguala las longitudes de las secuencias y devuelve las entradas y salidas ajustadas.\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    x, y = [], []\n",
    "    for (x_item, y_item) in data_batch:\n",
    "        x.append(x_item)\n",
    "        y.append(torch.cat([torch.tensor([BOS_IDX]),\n",
    "                            y_item,\n",
    "                            torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
    "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
    "    return x, y[:, :-1], y[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y preprocesar los datos de entrenamiento en lotes\n",
    "\n",
    "train_loader = DataLoader(train_data, \n",
    "                        batch_size = batch_size, \n",
    "                        shuffle = True, \n",
    "                        collate_fn = generate_batch,\n",
    "                        num_workers = 4, \n",
    "                        pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_batch, dec_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_batch.shape, dec_batch.shape, target_batch.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
